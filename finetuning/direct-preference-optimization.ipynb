{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Resources\n",
        " - https://arxiv.org/abs/2305.18290\n",
        " - https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb\n",
        "\n",
        " I highly recommend Sebastian Raschka, he is a great explainer of these concepts(transformer/GPT) and has been really helpful over the years"
      ],
      "metadata": {
        "id": "C5UZYK1fR2Rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is DPO?\n",
        "An alternative to RLHF. Unlike RLHF, it doesn't train a separate reward model and has no need for policy optimization\n",
        "\n",
        "![DPO equation](https://camo.githubusercontent.com/e23f17264853fa0c72445b5f250098258930e5238d6c766db812964564772461/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f64706f2f332e776562703f313233)"
      ],
      "metadata": {
        "id": "ajAfns16Sn96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data\n",
        "!wget -O data/instruction-data-with-preference.json https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/27a6a7e64a97a07da2030ed8c291cbb3e1a4bd0a/ch07/04_preference-tuning-with-dpo/instruction-data-with-preference.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2mMVLUyWVyR",
        "outputId": "06b53fd6-d257-4bec-d008-212436fe7e9d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-08 15:24:31--  https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/27a6a7e64a97a07da2030ed8c291cbb3e1a4bd0a/ch07/04_preference-tuning-with-dpo/instruction-data-with-preference.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 386968 (378K) [text/plain]\n",
            "Saving to: ‘data/instruction-data-with-preference.json’\n",
            "\n",
            "data/instruction-da 100%[===================>] 377.90K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-11-08 15:24:31 (21.6 MB/s) - ‘data/instruction-data-with-preference.json’ saved [386968/386968]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBJUZiYcR1W1",
        "outputId": "b6df747f-a3fd-4bab-ef50-650a8fc8df53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "file_path = \"data/instruction-data-with-preference.json\"\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "print(\"Number of entries:\", len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vlGB_8_SUHsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ],
      "metadata": {
        "id": "XU2mlbfDYY0a"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format(data[0])\n",
        "print(model_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egbsLq1AYgHf",
        "outputId": "f956dd7e-4496-481c-f00f-cf18569f4eb3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Evaluate the following phrase by transforming it into the spelling given.\n",
            "\n",
            "### Input:\n",
            "freind --> friend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = int(len(data) * 0.85)  # 85% for training\n",
        "test_dataset = int(len(data) * 0.1)    # 10% for testing\n",
        "val_dataset = len(data) - train_dataset - test_dataset  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_dataset]\n",
        "test_data = data[train_dataset:train_dataset + test_dataset]\n",
        "val_data = data[train_dataset + test_dataset:]\n",
        "\n",
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egEU9pD6Y1JX",
        "outputId": "44ddeaa4-a60d-4d16-899c-75c68f54795f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def decode(token_ids, tokenizer):\n",
        "    ids_in_python_list = token_ids.flatten().tolist()\n",
        "    return tokenizer.decode(ids_in_python_list)\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            prompt = format(entry)\n",
        "            rejected_response = entry[\"rejected\"]\n",
        "            chosen_response = entry[\"chosen\"]\n",
        "\n",
        "            prompt_tokens = tokenizer.encode(prompt)\n",
        "            chosen_full_text = f\"{prompt}\\n\\n### Response:\\n{chosen_response}\"\n",
        "            rejected_full_text = f\"{prompt}\\n\\n### Response:\\n{rejected_response}\"\n",
        "            chosen_full_tokens = tokenizer.encode(chosen_full_text)\n",
        "            rejected_full_tokens = tokenizer.encode(rejected_full_text)\n",
        "\n",
        "            self.encoded_texts.append({\n",
        "                \"prompt\": prompt_tokens,\n",
        "                \"chosen\": chosen_full_tokens,\n",
        "                \"rejected\": rejected_full_tokens,\n",
        "            })\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "_6gNHtS2ZEk2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "def collate_fn(\n",
        "        batch,\n",
        "        pad_token_id=50256,\n",
        "        allowed_max_length = None,\n",
        "        mask_prompt_tokens=True,\n",
        "        device=\"cpu\"\n",
        "):\n",
        "    # Initialize lists to hold batch data\n",
        "    batch_data:dict[str, list[Tensor]] = {\n",
        "        \"prompt\": [],\n",
        "        \"chosen\": [],\n",
        "        \"rejected\": [],\n",
        "        \"rejected_mask\": [],\n",
        "        \"chosen_mask\": []\n",
        "\n",
        "    }\n",
        "\n",
        "   # Determine the longest sequence to set a common padding length\n",
        "    max_length_common = 0\n",
        "    if batch:\n",
        "        max_chosen = max(len(item[\"chosen\"])+1 for item in batch)\n",
        "        max_rejected = max(len(item[\"rejected\"])+1 for item in batch)\n",
        "        max_length_common = max(max_chosen, max_rejected)\n",
        "\n",
        "        # Process each item in the batch\n",
        "    for item in batch:\n",
        "        prompt = torch.tensor(item[\"prompt\"])\n",
        "        batch_data[\"prompt\"].append(prompt)\n",
        "\n",
        "        for key in [\"chosen\", \"rejected\"]:\n",
        "            # Adjust padding according to the common maximum length\n",
        "            sequence = item[key]\n",
        "            padded = sequence + [pad_token_id] * (max_length_common - len(sequence))\n",
        "            mask = torch.ones(len(padded)).bool()\n",
        "\n",
        "            # Set mask for all padding tokens to False\n",
        "            mask[len(sequence):] = False\n",
        "\n",
        "            # Set mask for all input tokens to False\n",
        "            # +2 sets the 2 newline (\"\\n\") tokens before \"### Response\" to False\n",
        "            if mask_prompt_tokens:\n",
        "                mask[:prompt.shape[0]+2] = False\n",
        "\n",
        "            batch_data[key].append(torch.tensor(padded))\n",
        "            batch_data[f\"{key}_mask\"].append(mask)\n",
        "\n",
        "\n",
        "    # Final processing\n",
        "    for key in [\"chosen\", \"rejected\", \"chosen_mask\", \"rejected_mask\"]:\n",
        "        # Stack all sequences into a tensor for the given key\n",
        "        tensor_stack = torch.stack(batch_data[key])\n",
        "\n",
        "        # Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            tensor_stack = tensor_stack[:, :allowed_max_length]\n",
        "\n",
        "        # Move to the specified device\n",
        "        # Ignore ValueError\n",
        "        batch_data[key] = tensor_stack.to(device) # type: ignore\n",
        "\n",
        "    return batch_data"
      ],
      "metadata": {
        "id": "i7YWPRVPaO4h"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "custom_collate_fn = partial(\n",
        "    collate_fn,\n",
        "    device=device,            # Put the data directly on a GPU if available\n",
        "    mask_prompt_tokens=True,  # This is optional\n",
        "    allowed_max_length=1024   # The supported context length of the model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_pX9iWhdiM8",
        "outputId": "88a2346f-0883-4e9f-c885-bff6c9c36e85"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tiktoken"
      ],
      "metadata": {
        "id": "V7PYOIVZeJgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    print(\"batch.keys:\", batch.keys())\n",
        "    print(\"\\n\", \"-\"*8, \"\\n\", \"PROMPT TOKENS:\\n\", decode(batch[\"prompt\"][0], tokenizer))\n",
        "    print(\"\\n\", \"-\"*8, \"\\n\", \"CHOSEN TOKENS:\\n\", decode(batch[\"chosen\"][0][batch[\"chosen_mask\"][0]], tokenizer))\n",
        "    print(\"\\n\", \"-\"*8, \"\\n\", \"REJECTED TOKENS:\\n\", decode(batch[\"rejected\"][0][batch[\"rejected_mask\"][0]], tokenizer))\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_1FALzdd0Km",
        "outputId": "3471a2c6-7bd8-4c9a-b464-fd2e563e4d06"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch.keys: dict_keys(['prompt', 'chosen', 'rejected', 'rejected_mask', 'chosen_mask'])\n",
            "\n",
            " -------- \n",
            " PROMPT TOKENS:\n",
            " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Evaluate the following phrase by transforming it into the spelling given.\n",
            "\n",
            "### Input:\n",
            "freind --> friend\n",
            "\n",
            " -------- \n",
            " CHOSEN TOKENS:\n",
            " ### Response:\n",
            "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n",
            "\n",
            " -------- \n",
            " REJECTED TOKENS:\n",
            " ### Response:\n",
            "The spelling of the given phrase \"freind\" is flat out wrong, get it together, the correct spelling is \"friend\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "\n",
        "finetuned_model_path = Path(data / \"gpt2-medium355M-sft.pth\")\n",
        "if not finetuned_model_path.exists():\n",
        "    # download model ln 31\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "model.load_state_dict(\n",
        "    torch.load(\n",
        "        finetuned_model_path,\n",
        "        map_location=torch.device(\"cpu\"),\n",
        "        weights_only=True\n",
        "    )\n",
        ")\n",
        "model.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "DRNwoUlNe2Er",
        "outputId": "eae9e05a-396d-4531-f9b2-b8fb37f589a3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-54-0df93b3addc4>, line 8)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-0df93b3addc4>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    # download model ln 31\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_dpo_loss(\n",
        "      model_chosen_logprobs,\n",
        "      model_rejected_logprobs,\n",
        "      reference_chosen_logprobs,\n",
        "      reference_rejected_logprobs,\n",
        "      beta=0.1,\n",
        "    ):\n",
        "    \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\n",
        "\n",
        "    Args:\n",
        "        policy_chosen_logprobs: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n",
        "        policy_rejected_logprobs: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n",
        "        reference_chosen_logprobs: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n",
        "        reference_rejected_logprobs: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n",
        "        beta: Temperature parameter for the DPO loss; typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.\n",
        "        label_smoothing: conservativeness for DPO loss.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of three tensors: (loss, chosen_rewards, rejected_rewards).\n",
        "    \"\"\"\n",
        "\n",
        "    model_logratios = model_chosen_logprobs - model_rejected_logprobs\n",
        "    reference_logratios = reference_chosen_logprobs - reference_rejected_logprobs\n",
        "    logits = model_logratios - reference_logratios\n",
        "\n",
        "    # DPO (Eq. 7 of https://arxiv.org/pdf/2305.18290.pdf)\n",
        "    losses = -F.logsigmoid(beta * logits)\n",
        "\n",
        "    # Optional values to track progress during training\n",
        "    chosen_rewards = (model_chosen_logprobs - reference_chosen_logprobs).detach()\n",
        "    rejected_rewards = (model_rejected_logprobs - reference_rejected_logprobs).detach()\n",
        "\n",
        "    # .mean() to average over the samples in the batch\n",
        "    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()"
      ],
      "metadata": {
        "id": "aX4T9a1xiU3L"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}