{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5495556f-440f-4ce6-a5ef-0e8cadf05b0f",
   "metadata": {},
   "source": [
    "trying to understand basic cnn works as well as tensorboard\n",
    "from \n",
    "- Conv2d layer\n",
    "- MaxPooling\n",
    "- * tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ca69c9a-9b20-48ee-8c08-865613650763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('../../.tensor_board/data',\n",
    "  # download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST('../../.tensor_board/data',\n",
    "   # download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beeee1ac-b004-4ca4-8892-4e600b8e89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ebd255-78e2-4066-8f3b-db5bc2dd3a7a",
   "metadata": {},
   "source": [
    "Reimplemeting the CNN Model by implementing Conv2d from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9ee5f682-8fa9-4652-a44c-2f5a0101cdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    # a poorman's conv2d layer(only forward pass tested). written just to understand how it works under the hood.\n",
    "    def __init__(self, C_in, C_out, kernel_size):\n",
    "        \"\"\"\n",
    "        B -> Batch size,\n",
    "        C_out -> Number of output channels\n",
    "\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        \"\"\"\n",
    "        super(Conv2d, self).__init__()\n",
    "        self.k = kernel_size\n",
    "        self.C_out = C_out\n",
    "        self.C_in = C_in\n",
    "        self.weight = torch.nn.Parameter(torch.randn(C_out, C_in, kernel_size, kernel_size))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(C_out))\n",
    "    def cross_correlation(self, inputs, weight, dim):\n",
    "        # this can be replaced by np.correlate / scipy.ndimage.correlate, they are more efficient.\n",
    "        # it uses fast fourier transform in its computation with is magnitude faster for larger arrays.\n",
    "        \n",
    "        res = torch.zeros(dim)\n",
    "        \n",
    "        for i in range(dim[0]): # height\n",
    "            for j in range(dim[1]): # width \n",
    "                res[i,j] =  torch.sum(weight * inputs[i:i+self.k, j:j+self.k])\n",
    "        return res # (H_out, W_out)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input -> (B, C_in, H_in, W_in)\n",
    "        B = input.size(0)\n",
    "        # Calculate the output dimensions\n",
    "        H_out = input.size(-2) - self.k + 1\n",
    "        W_out = input.size(-1) - self.k + 1\n",
    "        \n",
    "        ans = torch.zeros(B, self.C_out, H_out, W_out) # (B, C_out, H, W)\n",
    "        for N_i in range(B):\n",
    "            for C_out_j in range(self.C_out):\n",
    "                cross_corr_sum = torch.zeros(H_out, W_out)\n",
    "                for k in range(self.C_in):\n",
    "                    c = self.cross_correlation(input[N_i, k], self.weight[C_out_j, k], dim=(H_out, W_out))\n",
    "                    cross_corr_sum += c\n",
    "                ans[N_i, C_out_j] = self.bias[C_out_j] + cross_corr_sum \n",
    "        return ans\n",
    "\n",
    "net = Conv2d(1, 6, 3)\n",
    "dd = net(torch.randn(4, 1, 10, 10))\n",
    "print(dd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "d9fc0a4e-c7a8-4a96-ab27-84620ed2de15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaxPool2d(nn.Module):\n",
    "    # a poorman's MaxPool2d layer(only forward pass tested). written just to understand how it works under the hood.\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        \"\"\"\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#maxpool2d\n",
    "        \"\"\"\n",
    "        super(MaxPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "    def forward(self, inputs):\n",
    "        B, C, H_in, W_in = inputs.shape\n",
    "        # assuming stride is 1 on both B, C\n",
    "        H_out = (H_in // self.kernel_size) \n",
    "        W_out = (W_in // self.kernel_size) \n",
    "        out = torch.zeros(B, C, H_out, W_out)\n",
    "        for b in range(B):\n",
    "            for c in range(C):\n",
    "                for h in range(H_out):\n",
    "                    for w in range(W_out):\n",
    "                        posh = h*self.stride\n",
    "                        posw = w*self.stride\n",
    "                        block = inputs[b, c, posh:posh+self.kernel_size, posw:posw+self.kernel_size]\n",
    "                        # ans = block.reshape((-1,)).max(0)\n",
    "                        # ans = block.max(dim=1, keepdim=True).values.max(dim=0, keepdim=True).values\n",
    "                        out[b, c, h, w] = torch.max(block)\n",
    "        return out\n",
    "\n",
    "net = MaxPool2d(2, 2)\n",
    "inp = torch.randn(2, 1, 16, 16, requires_grad=True)\n",
    "orig = nn.MaxPool2d(2, 2)\n",
    "# orig(inp) == net(inp)\n",
    "torch.allclose(orig(inp), net(inp))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "b2a9f79b-5068-4df9-93b0-58461b0ef7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr/conv:  [-0.5 -0.5  0.   0.5  1.   4.5  2.5] [-0.5 -0.5  0.   0.5  1.   4.5  2.5] True\n",
      "5\n",
      "input shape:  torch.Size([4, 1, 28, 28])\n",
      "labels shape:  torch.Size([4])\n",
      "original conv parameters:  [torch.Size([6, 1, 5, 5]), torch.Size([6])]\n",
      "my conv parameters:  [torch.Size([6, 1, 5, 5]), torch.Size([6])]\n",
      "conv2d output shape:  torch.Size([4, 6, 24, 24]) torch.Size([4, 6, 24, 24])\n",
      "testing both:  False True\n",
      "output from max pool shape:  torch.Size([4, 1, 14, 14])\n",
      "my output from max pool shape:  torch.Size([4, 1, 14, 14])\n",
      "testing both:  True True\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(1, 5,5)\n",
    "y = (np.random.randn(3) // 0.5) * 0.5\n",
    "# cross-correlation, is just convolution without the flip of the kernel\n",
    "corr, conv = np.correlate(x, y, mode=\"full\"), np.convolve(x, np.flip(y))\n",
    "print(\"corr/conv: \", corr, conv, np.all(corr == conv))\n",
    "\n",
    "# it seems that output_size = input_size - kernel_size + 1\n",
    "\n",
    "\n",
    "for i, data in enumerate(trainloader, 5): \n",
    "    inputs, labels = data\n",
    "    print(i)\n",
    "    print(\"input shape: \", inputs.shape)\n",
    "    print(\"labels shape: \", labels.shape)\n",
    "    conv = nn.Conv2d(1, 6, 5)\n",
    "    print(\"original conv parameters: \", [i.shape for i in conv.parameters()])\n",
    "    mine = Conv2d(1, 6, 5)\n",
    "    mine.weight = conv.weight\n",
    "    mine.bias = conv.bias\n",
    "    print(\"my conv parameters: \", [i.shape for i in mine.parameters()])\n",
    "    conv_out = conv(inputs)\n",
    "    mine_out = mine(inputs)\n",
    "    print(\"conv2d output shape: \", conv_out.shape, mine_out.shape)\n",
    "    print(\"testing both: \", torch.allclose(conv_out, mine_out), torch.all(conv_out - mine_out < 1e-5).item()) # 💢\n",
    "    # though all are not equal they are close at aproximate error of ~E=10^-5\n",
    "\n",
    "    pool = nn.MaxPool2d(2,2)\n",
    "    pool_out = pool(inputs)\n",
    "    print(\"output from max pool shape: \", pool_out.shape)\n",
    "    mine = MaxPool2d(2,2)\n",
    "    mine_out = mine(inputs)\n",
    "    print(\"my output from max pool shape: \", mine_out.shape)\n",
    "    print(\"testing both: \", torch.allclose(pool_out, mine_out), torch.all(pool_out - mine_out < 1e-5).item()) # 💢\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "798faef2-7316-4ffb-9681-d1c9347d31ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0138,  0.5590, -0.5872,  0.1536,  0.8506, -0.2311, -0.5321, -0.2137,\n",
      "         -0.1066,  1.6171],\n",
      "        [-0.3043, -0.4339,  1.6755,  1.1117,  0.7554,  1.3151, -0.6031, -0.6163,\n",
      "          0.1046,  0.7804],\n",
      "        [-0.9335,  0.2672,  0.5056, -0.5916, -0.6055,  2.3247, -1.2898, -2.7218,\n",
      "          0.6508, -1.1789]])\n"
     ]
    }
   ],
   "source": [
    "s = torch.randn(3, 10)\n",
    "print(s)\n",
    "s[0,0] = s.max(dim=1, keepdim=True).values.max(dim=0, keepdim=True).values\n",
    "# s.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f104fe-4557-4b34-aaa1-4ff4b83c5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2defd56f-f62c-4ed6-b775-9a0e93ceb983",
   "metadata": {},
   "source": [
    "## Set up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36b2a5e1-af37-4a3b-bafb-d6186950932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('../../.tensor_board/tmp/runs/fashion_mnist_experiment_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95ae18a-212b-4a8e-9fee-d17b9ebaf9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b4d755-2ea8-4996-acdc-7451c62ed8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model graph to tensorboard\n",
    "writer.add_graph(net, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f85f1a5-1b27-4b69-9376-515b0e510c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f380bbf-2d86-4ec5-8810-5cf56ecbd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54ee7f19-32dc-4ae8-95e7-e605c7fe4d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                            plot_classes_preds(net, inputs, labels),\n",
    "                            global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5764704d-67ed-46ce-b35e-0853baa393e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. gets the probability predictions in a test_size x num_classes Tensor\n",
    "# 2. gets the preds in a test_size Tensor\n",
    "# takes ~10 seconds to run\n",
    "class_probs = []\n",
    "class_label = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        output = net(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_label.append(labels)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_label = torch.cat(class_label)\n",
    "\n",
    "# helper function\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_truth = test_label == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_truth,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# plot all the pr curves\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9eab3266-134e-48ee-ae21-289359be03a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of parameters: 44426\n",
      "size in float32:  177.704 KB\n"
     ]
    }
   ],
   "source": [
    "total_params= sum(p.numel() for p in net.parameters())\n",
    "print(\"total number of parameters:\", total_params)\n",
    "print(\"size in float32: \",(total_params * 4)/1e3, \"KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
